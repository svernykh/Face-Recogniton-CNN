{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9a4543",
   "metadata": {},
   "source": [
    "# Inference and Evaluation Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76141e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d24ca",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Checks 'Test' folder, populates it from 'Train' if needed, and generates 'test.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305f8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Test Data...\n",
      "Found 68 classes in Train dataset.\n",
      "Generating test.csv...\n",
      "Test CSV created with 67 samples.\n",
      "Verifying images in Test...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_test_data(train_dir='Train', test_dir='Test', train_csv='dataset.csv', test_csv='test.csv'):\n",
    "    print(\"Preparing Test Data...\")\n",
    "    \n",
    "    # 1. Load Train CSV to get class mapping (to ensure consistency)\n",
    "    if not os.path.exists(train_csv):\n",
    "        print(f\"Error: {train_csv} not found. Cannot ensure class consistency.\")\n",
    "        return None, None\n",
    "    \n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    unique_labels = sorted(df_train['label'].unique()) # Sort to ensure deterministic order if not provided\n",
    "    # However, app.py uses: unique_labels = df['label'].unique() (order of appearance)\n",
    "    # To be safe, we should use the exact logic from app.py or load the mapping if saved.\n",
    "    # Let's stick to the order in dataset.csv for consistency with trained models.\n",
    "    unique_labels_original = df_train['label'].unique()\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels_original)}\n",
    "    idx_to_label = {idx: label for idx, label in enumerate(unique_labels_original)}\n",
    "    \n",
    "    print(f\"Found {len(unique_labels_original)} classes in Train dataset.\")\n",
    "\n",
    "    # 2. Populate Test Directory if empty\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "        \n",
    "    # Check if Test folder is populated\n",
    "    test_classes = os.listdir(test_dir)\n",
    "    if len(test_classes) < len(unique_labels_original):\n",
    "        print(\"Populating Test folder by copying 1 image from each Train class...\")\n",
    "        for label in unique_labels_original:\n",
    "            src_class_dir = os.path.join(train_dir, label)\n",
    "            dst_class_dir = os.path.join(test_dir, label)\n",
    "            \n",
    "            if not os.path.exists(dst_class_dir):\n",
    "                os.makedirs(dst_class_dir)\n",
    "                \n",
    "            # Check if destination has images\n",
    "            if len(os.listdir(dst_class_dir)) == 0:\n",
    "                if os.path.exists(src_class_dir):\n",
    "                    images = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                    if images:\n",
    "                        # Pick the last one to minimize impact on training if we were moving, but we are copying\n",
    "                        img_to_copy = images[-1] \n",
    "                        shutil.copy(os.path.join(src_class_dir, img_to_copy), os.path.join(dst_class_dir, img_to_copy))\n",
    "    \n",
    "    # 3. Create Test CSV\n",
    "    print(\"Generating test.csv...\")\n",
    "    test_data = []\n",
    "    for root, dirs, files in os.walk(test_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                label = os.path.basename(root)\n",
    "                if label in label_to_idx:\n",
    "                    test_data.append({'gambar': file, 'label': label, 'label_idx': label_to_idx[label]})\n",
    "    \n",
    "    df_test = pd.DataFrame(test_data)\n",
    "    df_test.to_csv(test_csv, index=False)\n",
    "    print(f\"Test CSV created with {len(df_test)} samples.\")\n",
    "    \n",
    "    return df_test, idx_to_label\n",
    "\n",
    "def verify_images(directory):\n",
    "    print(f\"Verifying images in {directory}...\")\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with Image.open(path) as img:\n",
    "                        img.verify()\n",
    "                except (IOError, SyntaxError, UnidentifiedImageError) as e:\n",
    "                    print(f\"Corrupted image found and removed: {path} ({e})\")\n",
    "                    os.remove(path)\n",
    "\n",
    "# Run Data Prep\n",
    "df_test, idx_to_label = prepare_test_data()\n",
    "verify_images('Test')\n",
    "num_classes = len(idx_to_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4858987",
   "metadata": {},
   "source": [
    "## 2. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e21c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_name = row['gambar']\n",
    "        label_name = row['label']\n",
    "        label_idx = row['label_idx']\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, label_name, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224)) # Dummy\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b3e02",
   "metadata": {},
   "source": [
    "## 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a4ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. EfficientNet V2 M (from app.py)\n",
    "def get_efficientnet_model(num_classes):\n",
    "    print(\"Creating EfficientNet V2 M model...\")\n",
    "    weights = models.EfficientNet_V2_M_Weights.DEFAULT\n",
    "    model = models.efficientnet_v2_m(weights=weights)\n",
    "    \n",
    "    # Replicate the modification logic\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 2. Swin Transformer V2 Tiny (from notebook)\n",
    "def get_swin_model(num_classes):\n",
    "    print(\"Creating Swin Transformer V2 Tiny model...\")\n",
    "    # Note: Ensure torchvision version supports swin_v2_t\n",
    "    try:\n",
    "        weights = models.Swin_V2_T_Weights.DEFAULT\n",
    "        model = models.swin_v2_t(weights=weights)\n",
    "    except AttributeError:\n",
    "        # Fallback if V2 not available or naming differs, try V1 or check version\n",
    "        print(\"Warning: Swin_V2_T not found, trying Swin_T\")\n",
    "        weights = models.Swin_T_Weights.DEFAULT\n",
    "        model = models.swin_t(weights=weights)\n",
    "\n",
    "    model.head = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(model.head.in_features, num_classes)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e60b0",
   "metadata": {},
   "source": [
    "## 4. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e027c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, dataloader, device, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\n--- Results for {model_name} ---\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b00bf",
   "metadata": {},
   "source": [
    "## 5. Main Execution\n",
    "Loads models, runs inference, and prints metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abfd9e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing EfficientNet V2 M...\n",
      "Creating EfficientNet V2 M model...\n",
      "Weights loaded successfully.\n",
      "Evaluating EfficientNet V2 M...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating EfficientNet V2 M: 100%|██████████| 3/3 [00:35<00:00, 11.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for EfficientNet V2 M ---\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1 Score:  1.0000\n",
      "\n",
      "Processing Swin Transformer V2 Tiny...\n",
      "Creating Swin Transformer V2 Tiny model...\n",
      "Weights loaded successfully.\n",
      "Evaluating Swin Transformer V2 Tiny...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Swin Transformer V2 Tiny: 100%|██████████| 3/3 [00:12<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results for Swin Transformer V2 Tiny ---\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1 Score:  1.0000\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration for models\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"EfficientNet V2 M\",\n",
    "        \"path\": \"face_recognition_project/efficientnet_v2_m_finetuned.pth\",\n",
    "        \"img_size\": 480,\n",
    "        \"get_model\": get_efficientnet_model\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Swin Transformer V2 Tiny\",\n",
    "        \"path\": \"face_recognition_project/swin_v2_t_finetuned.pth\",\n",
    "        \"img_size\": 256,\n",
    "        \"get_model\": get_swin_model\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in models_config:\n",
    "    print(f\"\\nProcessing {config['name']}...\")\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 1. Setup Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((config['img_size'], config['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 2. Setup DataLoader\n",
    "    test_dataset = FaceTestDataset(df_test, 'Test', transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # 3. Load Model\n",
    "    model = config['get_model'](num_classes)\n",
    "    \n",
    "    if os.path.exists(config['path']):\n",
    "        try:\n",
    "            state_dict = torch.load(config['path'], map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\"Weights loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weights: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Weight file not found at {config['path']}\")\n",
    "        continue\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    # 4. Evaluate\n",
    "    labels, preds = evaluate_model(model, test_loader, device, config['name'])\n",
    "    results[config['name']] = {'labels': labels, 'preds': preds}\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
